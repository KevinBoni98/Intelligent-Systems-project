{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b85006-9fe1-4d36-9cee-f116dd00e296",
   "metadata": {},
   "source": [
    "## 2 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a37486-bc7f-4955-9c86-df24c4695713",
   "metadata": {},
   "source": [
    "### 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39f4d96d-5d3b-48fb-882d-b661bbe77a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b0ce5-b6d6-4e78-b28f-6848f78c1161",
   "metadata": {},
   "source": [
    "### 1) Regolazione del contrasto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56108001-f5bf-4cba-a053-b81ca679b43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new contrast: 0.2\n",
      "datasets/Playing-Cards-Object-Detection-Dataset/test/images/416724470_jpg.rf.b9479650b738fa89f1eff26bcd5ea65c.jpg -> contrasto 0.200 (ok)\n",
      "new contrast: 0.1568627450980392\n",
      "datasets/Playing-Cards-Object-Detection-Dataset/valid/images/349250549_jpg.rf.74ea2da58134f73aa9a3d9f4531ca9bd.jpg -> contrasto 0.157 (ok)\n",
      "new contrast: 0.1843137254901961\n",
      "datasets/Playing-Cards-Object-Detection-Dataset/train/images/873020013_jpg.rf.b4818103a66109a0dd85699556ad4ee1.jpg -> contrasto 0.184 (ok)\n",
      "\n",
      "Immagini totali: 3\n",
      "Corrette (contrast < 0.08): 0\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION PARAMETERS ===\n",
    "# Modificare: quando si modificherà il csv dei dataset, fare che lo scorre e prende in automatico\n",
    "# il path delle immagini con contrasto sotto la soglia\n",
    "images = [\n",
    "    \"datasets/Playing-Cards-Object-Detection-Dataset/test/images/416724470_jpg.rf.b9479650b738fa89f1eff26bcd5ea65c.jpg\",\n",
    "    \"datasets/Playing-Cards-Object-Detection-Dataset/valid/images/349250549_jpg.rf.74ea2da58134f73aa9a3d9f4531ca9bd.jpg\",\n",
    "    \"datasets/Playing-Cards-Object-Detection-Dataset/train/images/873020013_jpg.rf.b4818103a66109a0dd85699556ad4ee1.jpg\"\n",
    "]\n",
    "OUTPUT_DIR = Path('./out')\n",
    "THRESHOLD = 0.08      # low contrast threshold\n",
    "METHOD = 'clahe'       # 'clahe', 'stretch', 'both'\n",
    "CLIP_LIMIT = 3.0\n",
    "TILE = 50\n",
    "# ================================\n",
    "\n",
    "def image_contrast(L: np.ndarray) -> float:\n",
    "    p2, p98 = np.percentile(L, (2, 98))\n",
    "    return (p98 - p2) / 255.0  # relative contrast\n",
    "\n",
    "def stretch_channel(L: np.ndarray, low: float, high: float) -> np.ndarray:\n",
    "    L = L.astype(np.float32)\n",
    "    denom = max(high - low, 1e-6)\n",
    "    L = (L - low) * (255.0 / denom)\n",
    "    return np.clip(L, 0, 255).astype(np.uint8)\n",
    "\n",
    "def apply_contrast(img: np.ndarray, method: str, clip_limit: float, tile: int) -> np.ndarray:\n",
    "    if img.ndim == 2 or img.shape[2] == 1:\n",
    "        L = img if img.ndim == 2 else img[..., 0]\n",
    "        if method in ('stretch', 'both'):\n",
    "            p2, p98 = np.percentile(L, (2, 98))\n",
    "            L = stretch_channel(L, p2, p98)\n",
    "        if method in ('clahe', 'both'):\n",
    "            clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(tile, tile))\n",
    "            L = clahe.apply(L)\n",
    "        return L\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    L, a, b = cv2.split(lab)\n",
    "    if method in ('stretch', 'both'):\n",
    "        p2, p98 = np.percentile(L, (2, 98))\n",
    "        L = stretch_channel(L, p2, p98)\n",
    "    if method in ('clahe', 'both'):\n",
    "        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(tile, tile))\n",
    "        L = clahe.apply(L)\n",
    "    lab = cv2.merge([L, a, b])\n",
    "    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def process_image(in_path: Path, out_root: Path) -> tuple[bool, float]:\n",
    "    img = cv2.imread(str(in_path), cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        return (False, 0.0)\n",
    "    out_path = out_root / Path(in_path).name\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if img.ndim == 2 or (img.ndim == 3 and img.shape[2] == 1):\n",
    "        L = img if img.ndim == 2 else img[..., 0]\n",
    "    else:\n",
    "        L = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)[:, :, 0]\n",
    "\n",
    "    contrast = image_contrast(L)\n",
    "    print(f\"new contrast: {contrast}\")\n",
    "    if contrast >= THRESHOLD:\n",
    "        return (False, contrast)\n",
    "\n",
    "    out = apply_contrast(img, METHOD, CLIP_LIMIT, TILE)\n",
    "    cv2.imwrite(str(out_path), out)\n",
    "    return (True, contrast)\n",
    "\n",
    "# === EXECUTION ===\n",
    "total = len(images)\n",
    "corrected = 0\n",
    "\n",
    "for img_path in images:\n",
    "    saved, c = process_image(Path(img_path), OUTPUT_DIR)\n",
    "    if saved:\n",
    "        corrected += 1\n",
    "    print(f\"{img_path} -> contrasto {c:.3f} {'(corretto)' if saved else '(ok)'}\")\n",
    "\n",
    "print(f\"\\nImmagini totali: {total}\")\n",
    "print(f\"Corrette (contrast < {THRESHOLD}): {corrected}\")\n",
    "# ================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94ffb1-7a92-41c3-bec1-334cb3fb8d08",
   "metadata": {},
   "source": [
    "### 2) Immagini con dimensioni differenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ceb8f-3e24-40e1-9a54-83a4ccbfca90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52768f2e-2873-48e8-a3c3-10d4bee2f5cc",
   "metadata": {},
   "source": [
    "### 3) Bounding Box\n",
    "Eliminare le label con bounding box:\n",
    "- più piccole dell'1% dell'immagine.\n",
    "- più grandi del 90% dell'immagine.\n",
    "- fuori dai bordi dell'immagine.\n",
    "Se l'immagine rimane senza label allora eliminarla tutta.\n",
    "\n",
    "Cercare di avere una buona distribuzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c3e68-2e72-4894-95a0-f43afc10c206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af68b8b3-7b6e-4b5d-a581-42f72f09a057",
   "metadata": {},
   "source": [
    "### 4) Correggere classi con correlazione troppo grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b588a0-fb22-4725-a46b-f154c9d09403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "934e643a-d695-434c-922d-720886118a73",
   "metadata": {},
   "source": [
    "### 5) Correzione Label\n",
    "Controllare se ci sono immagini senza label e il motivo: l'autore del dataset si è sbagliato, oppure perché non c'è alcuna carta. Nel primo caso mettiamo la label, nel secondo cancelliamo l'immagine. \n",
    "\n",
    "Inoltre, bisogna controllare anche se il dataset è bilanciato, quindi avere circa lo stesso numero di immagini per ogni label. Questo task è da fare per ultimo perché dipende dai precedenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e83705d-ea5e-4d4c-84a2-2d6170891e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
